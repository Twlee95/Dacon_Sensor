{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from sklearn.preprocessing import MaxAbsScaler, Normalizer, RobustScaler, StandardScaler,MinMaxScaler\n",
    "from sklearn.model_selection import KFold, StratifiedKFold      # St for class\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "from pytorch_tabnet.metrics import Metric\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set visible device\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"1, 2, 3\"\n",
    "\n",
    "CONFIG = {\n",
    "    'n_worker':16,\n",
    "    # Tabnet model\n",
    "    'epochs' : 100,\n",
    "    'patience' : 20,\n",
    "    'learning_rate':1e-3,\n",
    "    'weight_decay':1e-5,\n",
    "    'threshold':0.5,\n",
    "    'seed':42,\n",
    "    'fold':5\n",
    "}\n",
    "\n",
    "# seed setting function\n",
    "def seed_everything(seed:int):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CONFIG['seed']) # Seed setting\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= Norm \n",
    "def norm_transform(datatype, data, scaler_name='z-score', scaler=None):\n",
    "    scaler_dict = {\n",
    "        'z-score':StandardScaler(),\n",
    "        'minmax':MinMaxScaler(),\n",
    "        'maxabs':MaxAbsScaler(),\n",
    "        'robust':RobustScaler(),\n",
    "        'norm':Normalizer()\n",
    "    }\n",
    "    \n",
    "    # use only train\n",
    "    if not datatype==\"test\":\n",
    "        scaler = scaler_dict[scaler_name]\n",
    "        scaled_train = scaler.fit_transform(data)\n",
    "        return scaled_train, scaler\n",
    "    else:\n",
    "        scaled_test = scaler.transform(data)\n",
    "        return scaled_test\n",
    "\n",
    "# ============= pca \n",
    "def pca_transform(datatype, data, n_comp=300, pca=None):\n",
    "    if not datatype==\"test\":\n",
    "        pca = PCA(n_components=n_comp, random_state=CONFIG[\"seed\"])\n",
    "        pca_train = pca.fit_transform(data)\n",
    "        print(f\"with {n_comp} components, pca variance ratio : {sum(pca.explained_variance_ratio_)}\")\n",
    "        return pca_train, pca\n",
    "    else:\n",
    "        pca_test = pca.transform(data)\n",
    "        return pca_test\n",
    "\n",
    "\n",
    "def lg_nrmse(gt, preds):\n",
    "    # 각 Y Feature별 NRMSE 총합\n",
    "    # Y_01 ~ Y_08 까지 20% 가중치 부여\n",
    "    all_nrmse = []\n",
    "    for idx in range(0,14): # ignore 'ID'\n",
    "        rmse = mean_squared_error(gt[:,idx], preds[:,idx], squared=False)\n",
    "        nrmse = rmse/np.mean(np.abs(gt[:,idx]))\n",
    "        all_nrmse.append(nrmse)\n",
    "    score = 1.2 * np.sum(all_nrmse[:8]) + 1.0 * np.sum(all_nrmse[8:14])\n",
    "    return score\n",
    "\n",
    "    \n",
    "class NRMSE(Metric):\n",
    "    def __init__(self):\n",
    "        self._name = \"NormRMSE\"\n",
    "        self._maximize = False\n",
    "\n",
    "    def __call__(self, y_true, y_score):\n",
    "        nrmse = lg_nrmse(y_true, y_score)\n",
    "        return nrmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dada load and feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import FALSE\n",
    "\n",
    "\n",
    "MODEL_DIR_NAME = \"./Tabnet_models\"\n",
    "\n",
    "TABNET_OUTPUT_DIR_NAME = \"./tabnet_outputs\"\n",
    "SCALER_PATH = os.path.join(TABNET_OUTPUT_DIR_NAME, \"x_scaler.pkl\")\n",
    "\n",
    "if not os.path.exists(MODEL_DIR_NAME):\n",
    "    os.makedirs(MODEL_DIR_NAME)\n",
    "\n",
    "n_targets = 14\n",
    "\n",
    "df_train = pd.read_csv(\"datasets/train.csv\")\n",
    "df_test = pd.read_csv(\"datasets/test.csv\")\n",
    "\n",
    "df_train = df_train.drop('X_10', axis=1)\n",
    "df_train = df_train.drop('X_11', axis=1)\n",
    "df_test = df_test.drop('X_10', axis=1)\n",
    "df_test = df_test.drop('X_11', axis=1)\n",
    "\n",
    "\n",
    "\n",
    "df_train_x = df_train.iloc[:,1:-14].values\n",
    "df_train_y = df_train.iloc[:,-14:].values\n",
    "\n",
    "X_test_features = df_test.iloc[:,1:].values\n",
    "\n",
    "\n",
    "X_train_norm_features, scaler = norm_transform(\"train\", df_train_x, \"minmax\")\n",
    "\n",
    "with open(SCALER_PATH, \"wb\") as fw:\n",
    "    pickle.dump(scaler, fw)\n",
    "X_test_norm_features = norm_transform(\"test\", X_test_features, \"minmax\", scaler)\n",
    "\n",
    "\n",
    "#### generate k-menas feature ### \n",
    "K_Means_model = KMeans(n_clusters = 6, random_state = 10)\n",
    "\n",
    "K_Means_model.fit(X_train_norm_features)\n",
    "\n",
    "\n",
    "X_train_cluster_feature = K_Means_model.fit_predict(X_train_norm_features)\n",
    "X_test_cluster_feature = K_Means_model.fit_predict(X_test_norm_features)\n",
    "\n",
    "\n",
    "\n",
    "# ### one-hot encoding\n",
    "# oh_enc = OneHotEncoder(handle_unknown='ignore',sparse=False)\n",
    "# oh_train_cluster_feature = oh_enc.fit_transform(X_train_cluster_feature.reshape(-1, 1))\n",
    "# oh_test_cluster_feature = oh_enc.transform(X_test_cluster_feature.reshape(-1, 1))\n",
    "\n",
    "\n",
    "\n",
    "# X_train_features = np.concatenate((X_train_norm_features,oh_train_cluster_feature),axis=1)\n",
    "\n",
    "# X_test_features = np.concatenate((X_test_norm_features,oh_test_cluster_feature),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_norm_features_df =  pd.DataFrame(X_train_norm_features, columns = [ \"X_\" + str(i+1) for i in range(X_train_norm_features.shape[1])])\n",
    "X_test_norm_features_df =  pd.DataFrame(X_test_norm_features, columns = [ \"X_\" + str(i+1) for i in range(X_train_norm_features.shape[1])])\n",
    "\n",
    "X_train_cluster_feature = pd.DataFrame(X_train_cluster_feature, columns = [\"kmeans\"])\n",
    "X_test_cluster_feature = pd.DataFrame(X_test_cluster_feature, columns = [\"kmeans\"])\n",
    "\n",
    "X_train_cluster_feature = X_train_cluster_feature.kmeans.astype(\"object\")\n",
    "X_test_cluster_feature = X_test_cluster_feature.kmeans.astype(\"object\")\n",
    "\n",
    "X_train_features = pd.concat([X_train_norm_features_df,X_train_cluster_feature],axis=1)\n",
    "X_test_features = pd.concat([X_test_norm_features_df,X_test_cluster_feature],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kmeans 6\n",
      "0        1\n",
      "1        3\n",
      "2        2\n",
      "3        1\n",
      "4        1\n",
      "        ..\n",
      "39602    2\n",
      "39603    3\n",
      "39604    2\n",
      "39605    1\n",
      "39606    1\n",
      "Name: kmeans, Length: 39607, dtype: int32\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "l_enc = LabelEncoder()\n",
    "\n",
    "nunique = X_train_features.nunique()\n",
    "types = X_train_features.dtypes\n",
    "\n",
    "categorical_columns = []\n",
    "categorical_dims =  {}\n",
    "for col in X_train_features.columns:\n",
    "    if types[col] == 'object':\n",
    "        print(col, X_train_features[col].nunique())\n",
    "        X_train_features[col] = l_enc.fit_transform(X_train_features[col].values)\n",
    "        print(X_train_features[col])\n",
    "        categorical_columns.append(col)\n",
    "        categorical_dims[col] = len(l_enc.classes_)\n",
    "        print(categorical_dims[col])\n",
    "    # else:\n",
    "    #     X_train_features.fillna(X_train_features.loc[train_indices, col].mean(), inplace=True)\n",
    "\n",
    "\n",
    "# Categorical Embedding을 위해 Categorical 변수의 차원과 idxs를 담음.\n",
    "unused_feat = ['Set'] \n",
    "\n",
    "features = [ col for col in X_train_features.columns if col not in unused_feat+['target']]  # 7 target\n",
    "cat_idxs = [ i for i, f in enumerate(features) if f in categorical_columns]\n",
    "cat_dims = [ categorical_dims[f] for i, f in enumerate(features) if f in categorical_columns]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tabnet params\n",
    "tabnet_params = dict(\n",
    "    cat_idxs=cat_idxs,  # cat_idxs: List[int] = field(default_factory=list)\n",
    "    cat_dims=cat_dims,  # cat_dims: List[int] = field(default_factory=list)\n",
    "    cat_emb_dim=10,  \n",
    "    n_d = 64,   # 8 to 64\n",
    "    n_a = 128,  # n_d = n_a usally good\n",
    "    n_steps = 1,\n",
    "    gamma = 1.3,\n",
    "    lambda_sparse = 0,\n",
    "    n_independent = 2,\n",
    "    n_shared = 1,\n",
    "    optimizer_fn = optim.NAdam,\n",
    "    optimizer_params = dict(lr = CONFIG['learning_rate'], weight_decay = CONFIG['weight_decay']),\n",
    "    mask_type = \"entmax\",\n",
    "    scheduler_params = dict(\n",
    "        mode = \"min\", patience = 5, min_lr = 1e-5, factor = 0.9),\n",
    "    scheduler_fn = ReduceLROnPlateau,\n",
    "    seed = CONFIG[\"seed\"],\n",
    "    verbose = 5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TabNetRegressor() takes no arguments",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\lab\\JupyterProjects_lab\\lg_sensor\\tabnet-reg.ipynb 셀 12\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lab/JupyterProjects_lab/lg_sensor/tabnet-reg.ipynb#X14sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(LOG_PATH, \u001b[39m\"\u001b[39m\u001b[39ma\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m fa:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lab/JupyterProjects_lab/lg_sensor/tabnet-reg.ipynb#X14sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     fa\u001b[39m.\u001b[39mwrite(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m: ========= FOLD \u001b[39m\u001b[39m{\u001b[39;00mfold\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m ========= :\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/lab/JupyterProjects_lab/lg_sensor/tabnet-reg.ipynb#X14sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m model \u001b[39m=\u001b[39m TabNetRegressor(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtabnet_params)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lab/JupyterProjects_lab/lg_sensor/tabnet-reg.ipynb#X14sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m model\u001b[39m.\u001b[39mfit(X_train\u001b[39m=\u001b[39mX_train_features[train_idx], y_train\u001b[39m=\u001b[39mdf_train_y[train_idx],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lab/JupyterProjects_lab/lg_sensor/tabnet-reg.ipynb#X14sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m             eval_set\u001b[39m=\u001b[39m[(X_train_features[test_idx],df_train_y[test_idx])],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lab/JupyterProjects_lab/lg_sensor/tabnet-reg.ipynb#X14sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m             max_epochs\u001b[39m=\u001b[39mCONFIG[\u001b[39m'\u001b[39m\u001b[39mepochs\u001b[39m\u001b[39m'\u001b[39m], patience\u001b[39m=\u001b[39mCONFIG[\u001b[39m'\u001b[39m\u001b[39mpatience\u001b[39m\u001b[39m'\u001b[39m], \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lab/JupyterProjects_lab/lg_sensor/tabnet-reg.ipynb#X14sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m             eval_metric\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mrmse\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmse\u001b[39m\u001b[39m'\u001b[39m, NRMSE])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lab/JupyterProjects_lab/lg_sensor/tabnet-reg.ipynb#X14sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m model_name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m./tabnet_fold\u001b[39m\u001b[39m{\u001b[39;00mfold\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: TabNetRegressor() takes no arguments"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train_features = X_train_features.values\n",
    "\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=CONFIG['fold'], random_state=CONFIG['seed'], shuffle=True)\n",
    "\n",
    "avg_loss, avg_nrmse = 0, 0\n",
    "LOG_PATH = os.path.join(MODEL_DIR_NAME, \"log.txt\")\n",
    "with open(LOG_PATH, \"w\") as fw:\n",
    "    fw.write(\"Tabnet model ==\")\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(X_train_features)):\n",
    "    with open(LOG_PATH, \"a\") as fa:\n",
    "        fa.write(f\": ========= FOLD {fold+1} ========= :\\n\")\n",
    "\n",
    "    model = TabNetRegressor(**tabnet_params)\n",
    "    model.fit(X_train=X_train_features[train_idx], y_train=df_train_y[train_idx],\n",
    "                eval_set=[(X_train_features[test_idx],df_train_y[test_idx])],\n",
    "                max_epochs=CONFIG['epochs'], patience=CONFIG['patience'], \n",
    "                eval_metric=['rmse', 'mse', NRMSE])\n",
    "    \n",
    "    \n",
    "    model_name = f'./tabnet_fold{fold+1}'\n",
    "    model_path = os.path.join(MODEL_DIR_NAME, model_name)\n",
    "    model.save_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train y_hat 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds_reg = np.zeros((len(df_train), n_targets))\n",
    "\n",
    "# for fold in range(CONFIG['fold']):\n",
    "#     model_path = os.path.join(MODEL_DIR_NAME, f\"tabnet_fold{fold+1}.zip\")\n",
    "#     infer_model = TabNetRegressor(**tabnet_params)\n",
    "#     infer_model.load_model(model_path)\n",
    "    \n",
    "\n",
    "#     preds_reg += infer_model.predict(X_train_norm_features)\n",
    "#     print(len(preds_reg))\n",
    "    \n",
    "# preds_reg /= CONFIG['fold']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAMPLE_SUB_PATH = TABNET_OUTPUT_DIR_NAME+'/sample_submission.csv'\n",
    "\n",
    "# preds_reg = pd.DataFrame(preds_reg)\n",
    "# preds_reg.to_csv('./tabnet_outputs/tabnet_train_yhat.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kmeans 6\n"
     ]
    }
   ],
   "source": [
    "nunique = X_test_features.nunique()\n",
    "types = X_test_features.dtypes\n",
    "\n",
    "\n",
    "categorical_columns = []\n",
    "categorical_dims =  {}\n",
    "for col in X_test_features.columns:\n",
    "    if types[col] == 'object':\n",
    "        print(col, X_test_features[col].nunique())\n",
    "        X_test_features[col] = l_enc.fit_transform(X_test_features[col].values)\n",
    "        categorical_columns.append(col)\n",
    "        categorical_dims[col] = len(l_enc.classes_)\n",
    "\n",
    "\n",
    "unused_feat = ['Set']\n",
    "features = [ col for col in X_test_features.columns if col not in unused_feat+['target']]  # 7 target\n",
    "cat_idxs = [ i for i, f in enumerate(features) if f in categorical_columns]\n",
    "cat_dims = [ categorical_dims[f] for i, f in enumerate(features) if f in categorical_columns]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tabnet params\n",
    "tabnet_params = dict(\n",
    "    cat_idxs=cat_idxs,  # cat_idxs: List[int] = field(default_factory=list)\n",
    "    cat_dims=cat_dims,  # cat_dims: List[int] = field(default_factory=list)\n",
    "    cat_emb_dim=10,  \n",
    "    n_d = 64,   # 8 to 64\n",
    "    n_a = 128,  # n_d = n_a usally good\n",
    "    n_steps = 1,\n",
    "    gamma = 1.3,\n",
    "    lambda_sparse = 0,\n",
    "    n_independent = 2,\n",
    "    n_shared = 1,\n",
    "    optimizer_fn = optim.NAdam,\n",
    "    optimizer_params = dict(lr = CONFIG['learning_rate'], weight_decay = CONFIG['weight_decay']),\n",
    "    mask_type = \"entmax\",\n",
    "    scheduler_params = dict(\n",
    "        mode = \"min\", patience = 5, min_lr = 1e-5, factor = 0.9),\n",
    "    scheduler_fn = ReduceLROnPlateau,\n",
    "    seed = CONFIG[\"seed\"],\n",
    "    verbose = 5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test y_hat 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n"
     ]
    }
   ],
   "source": [
    "X_test_features = X_test_features.values\n",
    "preds_reg = np.zeros((len(df_test), n_targets))\n",
    "\n",
    "for fold in range(CONFIG['fold']):\n",
    "    model_path = os.path.join(MODEL_DIR_NAME, f\"tabnet_fold{fold+1}.zip\")\n",
    "    infer_model = TabNetRegressor(**tabnet_params)\n",
    "    infer_model.load_model(model_path)\n",
    "\n",
    "    preds_reg += infer_model.predict(X_test_features)\n",
    "\n",
    "preds_reg /= CONFIG['fold']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SUB_PATH = TABNET_OUTPUT_DIR_NAME+'/sample_submission.csv'\n",
    "submit = pd.read_csv(SAMPLE_SUB_PATH)\n",
    "submit.iloc[:, 1:] = preds_reg\n",
    "submit.to_csv('./tabnet_outputs/tabnet_submit.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('taewon')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "013403e7ebf8f35ee0411721c7e4b108aa3c3f8cb903b89610d110413a68ec3f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
