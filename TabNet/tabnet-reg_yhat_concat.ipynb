{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from sklearn.preprocessing import MaxAbsScaler, Normalizer, RobustScaler, StandardScaler,MinMaxScaler\n",
    "from sklearn.model_selection import KFold, StratifiedKFold      # St for class\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "from pytorch_tabnet.metrics import Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set visible device\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"1, 2, 3\"\n",
    "\n",
    "CONFIG = {\n",
    "    'n_worker':16,\n",
    "    # Tabnet model\n",
    "    'epochs' : 100,\n",
    "    'patience' : 20,\n",
    "    'learning_rate':2e-3,\n",
    "    'weight_decay':1e-5,\n",
    "    'threshold':0.5,\n",
    "    'seed':42,\n",
    "    'fold':5\n",
    "}\n",
    "\n",
    "# seed setting function\n",
    "def seed_everything(seed:int):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CONFIG['seed']) # Seed setting\n",
    "\n",
    "# tabnet params\n",
    "tabnet_params = dict(\n",
    "    n_d = 64,   # 8 to 64\n",
    "    n_a = 128,  # n_d = n_a usally good\n",
    "    n_steps = 1,\n",
    "    gamma = 1.3,\n",
    "    lambda_sparse = 0,\n",
    "    n_independent = 2,\n",
    "    n_shared = 1,\n",
    "    optimizer_fn = optim.NAdam,\n",
    "    optimizer_params = dict(lr = CONFIG['learning_rate'], weight_decay = CONFIG['weight_decay']),\n",
    "    mask_type = \"entmax\",\n",
    "    scheduler_params = dict(\n",
    "        mode = \"min\", patience = 5, min_lr = 1e-5, factor = 0.9),\n",
    "    scheduler_fn = ReduceLROnPlateau,\n",
    "    seed = CONFIG[\"seed\"],\n",
    "    verbose = 5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= Norm \n",
    "def norm_transform(datatype, data, scaler_name='z-score', scaler=None):\n",
    "    scaler_dict = {\n",
    "        'z-score':StandardScaler(),\n",
    "        'minmax':MinMaxScaler(),\n",
    "        'maxabs':MaxAbsScaler(),\n",
    "        'robust':RobustScaler(),\n",
    "        'norm':Normalizer()\n",
    "    }\n",
    "    \n",
    "    # use only train\n",
    "    if not datatype==\"test\":\n",
    "        scaler = scaler_dict[scaler_name]\n",
    "        scaled_train = scaler.fit_transform(data)\n",
    "        return scaled_train, scaler\n",
    "    else:\n",
    "        scaled_test = scaler.transform(data)\n",
    "        return scaled_test\n",
    "\n",
    "# ============= pca \n",
    "def pca_transform(datatype, data, n_comp=300, pca=None):\n",
    "    if not datatype==\"test\":\n",
    "        pca = PCA(n_components=n_comp, random_state=CONFIG[\"seed\"])\n",
    "        pca_train = pca.fit_transform(data)\n",
    "        print(f\"with {n_comp} components, pca variance ratio : {sum(pca.explained_variance_ratio_)}\")\n",
    "        return pca_train, pca\n",
    "    else:\n",
    "        pca_test = pca.transform(data)\n",
    "        return pca_test\n",
    "\n",
    "\n",
    "def lg_nrmse(gt, preds):\n",
    "    # 각 Y Feature별 NRMSE 총합\n",
    "    # Y_01 ~ Y_08 까지 20% 가중치 부여\n",
    "    all_nrmse = []\n",
    "    for idx in range(0,14): # ignore 'ID'\n",
    "        rmse = mean_squared_error(gt[:,idx], preds[:,idx], squared=False)\n",
    "        nrmse = rmse/np.mean(np.abs(gt[:,idx]))\n",
    "        all_nrmse.append(nrmse)\n",
    "    score = 1.2 * np.sum(all_nrmse[:8]) + 1.0 * np.sum(all_nrmse[8:14])\n",
    "    return score\n",
    "\n",
    "    \n",
    "class NRMSE(Metric):\n",
    "    def __init__(self):\n",
    "        self._name = \"NormRMSE\"\n",
    "        self._maximize = False\n",
    "\n",
    "    def __call__(self, y_true, y_score):\n",
    "        nrmse = lg_nrmse(y_true, y_score)\n",
    "        return nrmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'tabnet_outputs/tabnet_train_yhat.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\lab\\JupyterProjects_lab\\lg_sensor\\tabnet-reg_yhat_concat.ipynb 셀 4\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lab/JupyterProjects_lab/lg_sensor/tabnet-reg_yhat_concat.ipynb#W3sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m df_test \u001b[39m=\u001b[39m df_test\u001b[39m.\u001b[39mdrop(\u001b[39m'\u001b[39m\u001b[39mX_10\u001b[39m\u001b[39m'\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lab/JupyterProjects_lab/lg_sensor/tabnet-reg_yhat_concat.ipynb#W3sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m df_test \u001b[39m=\u001b[39m df_test\u001b[39m.\u001b[39mdrop(\u001b[39m'\u001b[39m\u001b[39mX_11\u001b[39m\u001b[39m'\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/lab/JupyterProjects_lab/lg_sensor/tabnet-reg_yhat_concat.ipynb#W3sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m train_yhat_data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m\"\u001b[39;49m\u001b[39mtabnet_outputs/tabnet_train_yhat.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lab/JupyterProjects_lab/lg_sensor/tabnet-reg_yhat_concat.ipynb#W3sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m test_yhat_data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m\"\u001b[39m\u001b[39mtabnet_outputs/tabnet_test_yhat.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lab/JupyterProjects_lab/lg_sensor/tabnet-reg_yhat_concat.ipynb#W3sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m train_yhat_data \u001b[39m=\u001b[39m train_yhat_data\u001b[39m.\u001b[39mvalues\n",
      "File \u001b[1;32mc:\\Users\\lab\\anaconda3\\envs\\taewon\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lab\\anaconda3\\envs\\taewon\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    665\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    666\u001b[0m     dialect,\n\u001b[0;32m    667\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    677\u001b[0m )\n\u001b[0;32m    678\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 680\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\lab\\anaconda3\\envs\\taewon\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    572\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    574\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    577\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    578\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\lab\\anaconda3\\envs\\taewon\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    932\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 933\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32mc:\\Users\\lab\\anaconda3\\envs\\taewon\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1213\u001b[0m     mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1214\u001b[0m \u001b[39m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m \u001b[39m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[0;32m   1216\u001b[0m \u001b[39m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[1;32m-> 1217\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(  \u001b[39m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1218\u001b[0m     f,\n\u001b[0;32m   1219\u001b[0m     mode,\n\u001b[0;32m   1220\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1221\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1222\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1223\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1224\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1225\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1226\u001b[0m )\n\u001b[0;32m   1227\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1228\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\lab\\anaconda3\\envs\\taewon\\lib\\site-packages\\pandas\\io\\common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    785\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    786\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    787\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    788\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    790\u001b[0m             handle,\n\u001b[0;32m    791\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    792\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    793\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    794\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    795\u001b[0m         )\n\u001b[0;32m    796\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    797\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    798\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'tabnet_outputs/tabnet_train_yhat.csv'"
     ]
    }
   ],
   "source": [
    "MODEL_DIR_NAME = \"./Tabnet_models\"\n",
    "\n",
    "TABNET_OUTPUT_DIR_NAME = \"./tabnet_outputs\"\n",
    "SCALER_PATH = os.path.join(TABNET_OUTPUT_DIR_NAME, \"x_scaler.pkl\")\n",
    "\n",
    "if not os.path.exists(MODEL_DIR_NAME):\n",
    "    os.makedirs(MODEL_DIR_NAME)\n",
    "\n",
    "n_targets = 14\n",
    "\n",
    "df_train = pd.read_csv(\"datasets/train.csv\")\n",
    "df_test = pd.read_csv(\"datasets/test.csv\")\n",
    "\n",
    "df_train = df_train.drop('X_10', axis=1)\n",
    "df_train = df_train.drop('X_11', axis=1)\n",
    "df_test = df_test.drop('X_10', axis=1)\n",
    "df_test = df_test.drop('X_11', axis=1)\n",
    "\n",
    "\n",
    "train_yhat_data = pd.read_csv(\"tabnet_outputs/tabnet_train_yhat.csv\")\n",
    "test_yhat_data = pd.read_csv(\"tabnet_outputs/tabnet_test_yhat.csv\")\n",
    "\n",
    "\n",
    "train_yhat_data = train_yhat_data.values\n",
    "test_yhat_data = test_yhat_data.values\n",
    "\n",
    "\n",
    "X_train_features = np.load(\"autoencoder_output/autoencoder_feature.npy\")\n",
    "X_test_features = np.load(\"autoencoder_output/autoencoder_feature_test.npy\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train_features = np.concatenate((X_train_features,train_yhat_data),axis=1)\n",
    "X_test_features = np.concatenate((X_test_features,test_yhat_data),axis=1)\n",
    "\n",
    "df_train_y = df_train.iloc[:, -14:].values\n",
    "\n",
    "\n",
    "X_train_norm_features, scaler = norm_transform(\"train\", X_train_features, \"minmax\")\n",
    "\n",
    "\n",
    "with open(SCALER_PATH, \"wb\") as fw:\n",
    "    pickle.dump(scaler, fw)\n",
    "X_test_norm_features = norm_transform(\"test\", X_test_features, \"minmax\", scaler)\n",
    "\n",
    "kf = KFold(n_splits=CONFIG['fold'], random_state=CONFIG['seed'], shuffle=True)\n",
    "\n",
    "avg_loss, avg_nrmse = 0, 0\n",
    "LOG_PATH = os.path.join(MODEL_DIR_NAME, \"log.txt\")\n",
    "with open(LOG_PATH, \"w\") as fw:\n",
    "    fw.write(\"Tabnet model ==\")\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(X_train_norm_features)):\n",
    "    with open(LOG_PATH, \"a\") as fa:\n",
    "        fa.write(f\": ========= FOLD {fold+1} ========= :\\n\")\n",
    "\n",
    "    model = TabNetRegressor(**tabnet_params)\n",
    "    model.fit(X_train=X_train_norm_features[train_idx], y_train=df_train_y[train_idx],\n",
    "                eval_set=[(X_train_norm_features[test_idx],df_train_y[test_idx])],\n",
    "                max_epochs=CONFIG['epochs'], patience=CONFIG['patience'], \n",
    "                eval_metric=['rmse', 'mse', NRMSE])\n",
    "    \n",
    "    \n",
    "    model_name = f'./tabnet_fold{fold+1}'\n",
    "    model_path = os.path.join(MODEL_DIR_NAME, model_name)\n",
    "    model.save_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test y_hat 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n"
     ]
    }
   ],
   "source": [
    "preds_reg = np.zeros((len(df_test), n_targets))\n",
    "\n",
    "for fold in range(CONFIG['fold']):\n",
    "    model_path = os.path.join(MODEL_DIR_NAME, f\"tabnet_fold{fold+1}.zip\")\n",
    "    infer_model = TabNetRegressor(**tabnet_params)\n",
    "    infer_model.load_model(model_path)\n",
    "\n",
    "    preds_reg += infer_model.predict(X_test_norm_features)\n",
    "\n",
    "preds_reg /= CONFIG['fold']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SUB_PATH = TABNET_OUTPUT_DIR_NAME+'/sample_submission.csv'\n",
    "submit = pd.read_csv(SAMPLE_SUB_PATH)\n",
    "submit.iloc[:, 1:] = preds_reg\n",
    "submit.to_csv('./tabnet_outputs/tabnet_submit.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('taewon')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "013403e7ebf8f35ee0411721c7e4b108aa3c3f8cb903b89610d110413a68ec3f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
